issuetype in (Requirement, Epic)
AND (
  labels = "Baseline"
  OR issue in epicsOf(
       "project = RWA
        AND issuetype = Requirement
        AND labels = \"Baseline\""
     )
)

----
issuetype in (Requirement, Epic)
AND (
   labels = "YourLabel"
   OR
   issue in epicsOf(
     "project = YOUR_PROJECT_KEY
      AND issuetype = Requirement
      AND labels = \"YourLabel\""
   )
)



----

import os
import csv
import google.auth
from google.cloud import bigquery
from google.api_core.exceptions import NotFound

# ─── 1) Proxy (if you need one) ────────────────────────────────────────────────
os.environ["HTTP_PROXY"]  = "googleapis-dev.gcp.cloud.uk.hsbc:3128"
os.environ["HTTPS_PROXY"] = "googleapis-dev.gcp.cloud.uk.hsbc:3128"

# ─── 2) Environment & Authentication ──────────────────────────────────────────
project     = "hsbc-9093058-rwapc52-dev"  # your GCP project ID
credentials, _ = google.auth.default()
client      = bigquery.Client(credentials=credentials, project=project)

# ─── 3) Upload Function ───────────────────────────────────────────────────────

def upload_csv_to_bq(
    csv_file_path: str,
    bq_table_name: str,
    *,
    force_refresh: bool = False
):
    """
    Load a local CSV into BigQuery.
     - If force_refresh=True, always truncates & reloads the table.
     - Else: append if schema matches; else truncate & reload.
    After an append, dedupe so you never get exact-row duplicates.
    """
    table_id = f"{project}.log_ds.{bq_table_name}"

    # 3.1) Check if the table already exists
    try:
        table = client.get_table(table_id)
        table_exists = True
    except NotFound:
        table_exists = False

    # 3.2) Read the CSV header row
    with open(csv_file_path, newline="") as csvfile:
        reader     = csv.reader(csvfile)
        csv_header = next(reader)

    # 3.3) Decide WriteDisposition
    if force_refresh or not table_exists:
        write_disp = bigquery.WriteDisposition.WRITE_TRUNCATE
    else:
        existing_cols = [f.name for f in table.schema]
        if existing_cols == csv_header:
            write_disp = bigquery.WriteDisposition.WRITE_APPEND
        else:
            write_disp = bigquery.WriteDisposition.WRITE_TRUNCATE

    # 3.4) Configure the load job
    job_config = bigquery.LoadJobConfig(
        skip_leading_rows   = 1,
        source_format       = bigquery.SourceFormat.CSV,
        autodetect          = True,
        create_disposition  = bigquery.CreateDisposition.CREATE_IF_NEEDED,
        write_disposition   = write_disp
    )

    # 3.5) Run the load
    with open(csv_file_path, "rb") as source_file:
        load_job = client.load_table_from_file(
            source_file,
            table_id,
            job_config=job_config
        )
    print(f"Started load job {load_job.job_id} for {csv_file_path} → {table_id} ({write_disp})")
    load_job.result()
    print("Load complete.")

    # 3.6) If we appended (and not forced), dedupe exact duplicates
    if (not force_refresh 
        and table_exists 
        and write_disp == bigquery.WriteDisposition.WRITE_APPEND):
        dedupe_sql = f"""
        CREATE OR REPLACE TABLE `{table_id}` AS
        SELECT DISTINCT *
        FROM `{table_id}`;
        """
        client.query(dedupe_sql).result()
        print("Deduplication complete.")

# ─── 4) Example Usage ─────────────────────────────────────────────────────────
if __name__ == "__main__":
    # full refresh:
    upload_csv_to_bq(
      "phase_durations_summary.csv",
      "RJ_jira_table_new",
      force_refresh=True
    )

    # intelligent append/truncate:
    upload_csv_to_bq(
      "jira_logs.csv",
      "RJ_jira_logs",
      force_refresh=False
    )